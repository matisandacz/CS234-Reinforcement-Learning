{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matisandacz/CS234-Reinforcement-Learning/blob/main/Multi_Armed_Bandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original Code: https://github.com/ankonzoid/LearningX/blob/master/classical_RL/multiarmed_bandit/multiarmed_bandit.py\n",
        "\n",
        "The code below adapts it to support M bandits"
      ],
      "metadata": {
        "id": "0u5QGN0JQ447"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym5VtTlSNavN",
        "outputId": "3ad7665f-c3ef-4a6f-d0c1-588871cd01e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running multi-armed bandits with nActions = 10, eps = 0.1\n",
            "Running 5000 experiments\n",
            "[Experiment 50/5000] n_steps = 500, reward_avg = 0.1474520487272523\n",
            "[Experiment 100/5000] n_steps = 500, reward_avg = 0.26158514876457356\n",
            "[Experiment 150/5000] n_steps = 500, reward_avg = 0.22187519529275454\n",
            "[Experiment 200/5000] n_steps = 500, reward_avg = 0.24634795499051407\n",
            "[Experiment 250/5000] n_steps = 500, reward_avg = 0.2894358048908714\n",
            "[Experiment 300/5000] n_steps = 500, reward_avg = 0.3259415358088585\n",
            "[Experiment 350/5000] n_steps = 500, reward_avg = 0.28030234941310495\n",
            "[Experiment 400/5000] n_steps = 500, reward_avg = 0.3266505289951367\n",
            "[Experiment 450/5000] n_steps = 500, reward_avg = 0.32110938694555863\n",
            "[Experiment 500/5000] n_steps = 500, reward_avg = 0.18638222218807562\n",
            "[Experiment 550/5000] n_steps = 500, reward_avg = 0.2546778493671617\n",
            "[Experiment 600/5000] n_steps = 500, reward_avg = 0.188414495940578\n",
            "[Experiment 650/5000] n_steps = 500, reward_avg = 0.22327016884385079\n",
            "[Experiment 700/5000] n_steps = 500, reward_avg = 0.2706785050107093\n",
            "[Experiment 750/5000] n_steps = 500, reward_avg = 0.21994792690843729\n",
            "[Experiment 800/5000] n_steps = 500, reward_avg = 0.23742012817279734\n",
            "[Experiment 850/5000] n_steps = 500, reward_avg = 0.3130826071631528\n",
            "[Experiment 900/5000] n_steps = 500, reward_avg = 0.16218908550462538\n",
            "[Experiment 950/5000] n_steps = 500, reward_avg = 0.20779460043264153\n",
            "[Experiment 1000/5000] n_steps = 500, reward_avg = 0.1843842561215356\n",
            "[Experiment 1050/5000] n_steps = 500, reward_avg = 0.26939870891635526\n",
            "[Experiment 1100/5000] n_steps = 500, reward_avg = 0.21576499559174883\n",
            "[Experiment 1150/5000] n_steps = 500, reward_avg = 0.2978939649285861\n",
            "[Experiment 1200/5000] n_steps = 500, reward_avg = 0.260776547824327\n",
            "[Experiment 1250/5000] n_steps = 500, reward_avg = 0.20554812438534617\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        " multiarmed_bandit.py  (author: Anson Wong / git: ankonzoid)\n",
        "\n",
        " We solve the multi-armed bandit problem using a classical epsilon-greedy\n",
        " agent with reward-average sampling as the estimate to action-value Q.\n",
        " This algorithm follows closely with the notation of Sutton's RL textbook.\n",
        "\n",
        " We set up bandit arms with fixed probability distribution of success,\n",
        " and receive stochastic rewards from each arm of +1 for success,\n",
        " and 0 reward for failure.\n",
        "\n",
        " The incremental update rule action-value Q for each (action a, reward r):\n",
        "   n += 1\n",
        "   Q(a) <- Q(a) + 1/n * (r - Q(a))\n",
        "\n",
        " where:\n",
        "   n = number of times action \"a\" was performed\n",
        "   Q(a) = value estimate of action \"a\"\n",
        "   r(a) = reward of sampling action bandit (bandit) \"a\"\n",
        "\n",
        " Derivation of the Q incremental update rule:\n",
        "   Q_{n+1}(a)\n",
        "   = 1/n * (r_1(a) + r_2(a) + ... + r_n(a))\n",
        "   = 1/n * ((n-1) * Q_n(a) + r_n(a))\n",
        "   = 1/n * (n * Q_n(a) + r_n(a) - Q_n(a))\n",
        "   = Q_n(a) + 1/n * (r_n(a) - Q_n(a))\n",
        "\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(0)\n",
        "\n",
        "class Environment:\n",
        "  \"\"\"This is a simulated environment.\n",
        "  In reality for model-free tasks, we would interact with a real environment\n",
        "\n",
        "  bandits: Number of bandits (Slot machines)\n",
        "  actions: Number of arms for each bandit\n",
        "  probs: Probability of success for each arm of each bandit.\n",
        "  This is an array containing the probabilities for winning for each arm of each bandit.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, probs):\n",
        "     # Success probabilities for each arm.\n",
        "     # In practice, we don't know these values. We want to find them.\n",
        "      self.gaussian_mean = 0\n",
        "      self.gaussian_variance = 1\n",
        "      self.probs = probs\n",
        "\n",
        "  \"\"\"\n",
        "  Performs a step in the environment.\n",
        "  You need to pick a bandit, and then you need to pick an action\n",
        "  \"\"\"\n",
        "  def step(self, bandit, action):\n",
        "      # Pull arm and get stochastic reward (1 for success, 0 for failure)\n",
        "      base_reward = 1 if (np.random.random()  < self.probs[bandit][action]) else 0\n",
        "      noise = np.random.normal(self.gaussian_mean, self.gaussian_variance) # Sample noise from Gaussian distribution\n",
        "      return base_reward + noise # Return reward with added noise\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, nBandits, nActions, eps):\n",
        "        self.nBandits = nBandits # Number of bandits\n",
        "        self.nActions = nActions # Number of actions\n",
        "        self.eps = eps # probability of exploration vs exploitation.\n",
        "        self.n = np.zeros((nBandits, nActions), dtype=int) # action counts n(bandit, a)\n",
        "        self.Q = np.zeros((nBandits, nActions), dtype=float) # value Q(bandit, a)\n",
        "\n",
        "    def update_Q(self, bandit, action, reward):\n",
        "        # Update Q action-value given (action, reward)\n",
        "        self.n[bandit][action] += 1\n",
        "        # A computation efficient strategy to store the value function.\n",
        "        # This is the average cumilative reward per action\n",
        "        self.Q[bandit][action] += (1.0/self.n[bandit][action]) * (reward - self.Q[bandit][action])\n",
        "\n",
        "    def get_action(self):\n",
        "        # Epsilon-greedy policy\n",
        "        if np.random.random() < self.eps: # explore\n",
        "            bandit = np.random.randint(self.nBandits)\n",
        "            action = np.random.randint(self.nActions)\n",
        "            return bandit, action\n",
        "        else: # exploit\n",
        "            # Find the indices of the maximum Q-value\n",
        "            max_indices = np.argwhere(self.Q == np.amax(self.Q))  # Get all indices of max Q-values\n",
        "            # Randomly select one of the maximum indices\n",
        "            chosen_index = np.random.choice(max_indices.shape[0])\n",
        "            bandit, action = max_indices[chosen_index]\n",
        "            return bandit, action\n",
        "\n",
        "\n",
        "def experiment(env, agent, probs, N_episodes):\n",
        "    \"\"\"Start multi-armed bandit simulation\"\"\"\n",
        "    actions, rewards = [], []\n",
        "    for episode in range(N_episodes):\n",
        "        bandit, action = agent.get_action() # sample policy # integer correcponding to the arm taken.\n",
        "        reward = env.step(bandit, action) # take step + get reward # integer 0/1\n",
        "        agent.update_Q(bandit, action, reward) # update Q\n",
        "        actions.append((bandit, action)) # list of tuples containing bandit action pairs\n",
        "        rewards.append(reward) # list of ints\n",
        "    return np.array(actions), np.array(rewards) # 500 x 1, 500 x 1\n",
        "\n",
        "def create_bandit_probabilities(nBandits, nActions):\n",
        "    \"\"\"\n",
        "    Creates an array of probabilities for winning for multiple bandits.\n",
        "\n",
        "    Args:\n",
        "      nBandits: The number of bandits.\n",
        "      nActions: The number of actions per bandit.\n",
        "\n",
        "    Returns:\n",
        "      A NumPy array of shape (nBandits, nActions) containing probabilities.\n",
        "    \"\"\"\n",
        "    probabilities = []\n",
        "    for _ in range(nBandits):\n",
        "      # Generate random numbers that will become probabilities\n",
        "      random_numbers = np.random.rand(nActions)\n",
        "      # Normalize the numbers to sum to 1 (creating a distribution)\n",
        "      probabilities.append(random_numbers / np.sum(random_numbers))\n",
        "\n",
        "    #probabilities[0][3] = 0.9\n",
        "    #probabilities[0][4] = 0.1\n",
        "    #for i in range(nActions):\n",
        "    #  if i != 3 and i != 4:\n",
        "    #    probabilities[0][i] = 0\n",
        "\n",
        "    return np.array(probabilities)\n",
        "\n",
        "# Settings\n",
        "probs = create_bandit_probabilities(10, 10)\n",
        "\n",
        "# This is the ground truth we don't know initially.\n",
        "N_steps = 500 # number of steps (per episode)\n",
        "N_experiments = 5000 # number of experiments to perform\n",
        "eps = 0.1 # probability of random exploration (fraction)\n",
        "save_fig = True # save file in same directory\n",
        "output_dir = os.path.join(os.getcwd(), \"output\")\n",
        "env = Environment(probs) # initialize arm probabilities\n",
        "agent = Agent(len(env.probs), len(env.probs[0]), eps)  # initialize agent\n",
        "####\n",
        "\n",
        "# Run multi-armed bandit experiments\n",
        "print(\"Running multi-armed bandits with nActions = {}, eps = {}\".format(len(probs), eps))\n",
        "R = np.zeros((N_steps,))  # reward history sum. 500 x 1\n",
        "A = np.zeros((N_steps, len(probs), len(probs[0])))  # action history sum 500 x nBandits x nAactions\n",
        "steps = np.zeros((N_experiments,)) # steps history 500 x 1\n",
        "avg_rewards = np.zeros((N_experiments,)) # average rewards history 500 x 1\n",
        "print(\"Running {} experiments\".format(N_experiments))\n",
        "for i in range(N_experiments):\n",
        "    actions, rewards = experiment(env, agent, probs, N_steps)  # perform experiment. actions = 500x1, rewards = 500x1\n",
        "    if i == 0:\n",
        "      steps[0] = N_steps\n",
        "    else:\n",
        "      steps[i] = steps[i-1] + N_steps\n",
        "    avg_rewards[i] = np.sum(rewards) / len(rewards)\n",
        "    if (i + 1) % (N_experiments / 100) == 0:\n",
        "        print(\"[Experiment {}/{}] \".format(i + 1, N_experiments) +\n",
        "              \"n_steps = {}, \".format(N_steps) +\n",
        "              \"reward_avg = {}\".format(np.sum(rewards) / len(rewards)))\n",
        "    R += rewards # Adding rewards for every time step across all experiments: 500x1\n",
        "    for j, a in enumerate(actions):\n",
        "        # step_index x num_actions = 500 x 10.\n",
        "        # Each cell holds number of actions a in time step j across all experiments\n",
        "        A[j][a[0]][a[1]] += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming A is your 3D array with shape (N_steps, N_bandits, N_actions)\n",
        "N_steps, N_bandits, N_actions = A.shape\n",
        "\n",
        "for bandit in range(N_bandits):\n",
        "    for action in range(N_actions):\n",
        "        print(f\"Bandit {bandit}, Action {action}: {np.sum(A[:, bandit, action])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7ye-CERGsN5",
        "outputId": "354482d0-b16b-426e-91ad-0069a859c59b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bandit 0, Action 0: 2482.0\n",
            "Bandit 0, Action 1: 2709.0\n",
            "Bandit 0, Action 2: 2501.0\n",
            "Bandit 0, Action 3: 2720.0\n",
            "Bandit 0, Action 4: 2540.0\n",
            "Bandit 0, Action 5: 2577.0\n",
            "Bandit 0, Action 6: 2505.0\n",
            "Bandit 0, Action 7: 2624.0\n",
            "Bandit 0, Action 8: 2527.0\n",
            "Bandit 0, Action 9: 2551.0\n",
            "Bandit 1, Action 0: 2948.0\n",
            "Bandit 1, Action 1: 2735.0\n",
            "Bandit 1, Action 2: 2854.0\n",
            "Bandit 1, Action 3: 2569.0\n",
            "Bandit 1, Action 4: 2565.0\n",
            "Bandit 1, Action 5: 2559.0\n",
            "Bandit 1, Action 6: 2706.0\n",
            "Bandit 1, Action 7: 2616.0\n",
            "Bandit 1, Action 8: 2481.0\n",
            "Bandit 1, Action 9: 2654.0\n",
            "Bandit 2, Action 0: 2614.0\n",
            "Bandit 2, Action 1: 2528.0\n",
            "Bandit 2, Action 2: 2610.0\n",
            "Bandit 2, Action 3: 2628.0\n",
            "Bandit 2, Action 4: 2536.0\n",
            "Bandit 2, Action 5: 2626.0\n",
            "Bandit 2, Action 6: 2504.0\n",
            "Bandit 2, Action 7: 2500.0\n",
            "Bandit 2, Action 8: 2541.0\n",
            "Bandit 2, Action 9: 2452.0\n",
            "Bandit 3, Action 0: 2547.0\n",
            "Bandit 3, Action 1: 2544.0\n",
            "Bandit 3, Action 2: 2431.0\n",
            "Bandit 3, Action 3: 2626.0\n",
            "Bandit 3, Action 4: 2475.0\n",
            "Bandit 3, Action 5: 2537.0\n",
            "Bandit 3, Action 6: 2640.0\n",
            "Bandit 3, Action 7: 2476.0\n",
            "Bandit 3, Action 8: 2465.0\n",
            "Bandit 3, Action 9: 2555.0\n",
            "Bandit 4, Action 0: 2422.0\n",
            "Bandit 4, Action 1: 2766.0\n",
            "Bandit 4, Action 2: 2429.0\n",
            "Bandit 4, Action 3: 2422.0\n",
            "Bandit 4, Action 4: 2482.0\n",
            "Bandit 4, Action 5: 2534.0\n",
            "Bandit 4, Action 6: 2543.0\n",
            "Bandit 4, Action 7: 2473.0\n",
            "Bandit 4, Action 8: 2562.0\n",
            "Bandit 4, Action 9: 2526.0\n",
            "Bandit 5, Action 0: 2523.0\n",
            "Bandit 5, Action 1: 2538.0\n",
            "Bandit 5, Action 2: 2975.0\n",
            "Bandit 5, Action 3: 2559.0\n",
            "Bandit 5, Action 4: 2573.0\n",
            "Bandit 5, Action 5: 2528.0\n",
            "Bandit 5, Action 6: 2480.0\n",
            "Bandit 5, Action 7: 2529.0\n",
            "Bandit 5, Action 8: 2620.0\n",
            "Bandit 5, Action 9: 2514.0\n",
            "Bandit 6, Action 0: 2509.0\n",
            "Bandit 6, Action 1: 2452.0\n",
            "Bandit 6, Action 2: 2976.0\n",
            "Bandit 6, Action 3: 2492.0\n",
            "Bandit 6, Action 4: 2472.0\n",
            "Bandit 6, Action 5: 2560.0\n",
            "Bandit 6, Action 6: 2586.0\n",
            "Bandit 6, Action 7: 2507.0\n",
            "Bandit 6, Action 8: 2209828.0\n",
            "Bandit 6, Action 9: 2546.0\n",
            "Bandit 7, Action 0: 5862.0\n",
            "Bandit 7, Action 1: 2656.0\n",
            "Bandit 7, Action 2: 2657.0\n",
            "Bandit 7, Action 3: 2459.0\n",
            "Bandit 7, Action 4: 2503.0\n",
            "Bandit 7, Action 5: 2575.0\n",
            "Bandit 7, Action 6: 2562.0\n",
            "Bandit 7, Action 7: 2586.0\n",
            "Bandit 7, Action 8: 2537.0\n",
            "Bandit 7, Action 9: 2557.0\n",
            "Bandit 8, Action 0: 2495.0\n",
            "Bandit 8, Action 1: 2712.0\n",
            "Bandit 8, Action 2: 2585.0\n",
            "Bandit 8, Action 3: 2801.0\n",
            "Bandit 8, Action 4: 2491.0\n",
            "Bandit 8, Action 5: 2773.0\n",
            "Bandit 8, Action 6: 2529.0\n",
            "Bandit 8, Action 7: 2477.0\n",
            "Bandit 8, Action 8: 2522.0\n",
            "Bandit 8, Action 9: 35156.0\n",
            "Bandit 9, Action 0: 2468.0\n",
            "Bandit 9, Action 1: 2464.0\n",
            "Bandit 9, Action 2: 2491.0\n",
            "Bandit 9, Action 3: 2773.0\n",
            "Bandit 9, Action 4: 2531.0\n",
            "Bandit 9, Action 5: 2464.0\n",
            "Bandit 9, Action 6: 2596.0\n",
            "Bandit 9, Action 7: 2547.0\n",
            "Bandit 9, Action 8: 2487.0\n",
            "Bandit 9, Action 9: 2500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yu9Kg84CPvDX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}